layout: post
title: "UTF-8 et encodage"
date: 2025-09-29 21:44:00 -0000
categories: CATEGORY-1 CATEGORY-2

# Comprendre l'encodage universel

On dit que **UTF-8 est l’encodage universel** parce qu’il peut représenter **tous les caractères définis dans Unicode**, de ton accent circonflexe jusqu’à l’emoji caca 💩, tout en restant compatible avec les anciens systèmes qui utilisaient l’ASCII.

Bref : **UTF-8 c’est le Wi-Fi illimité, le reste c’est le modem 56k.**

---
## 1. Unicode : le grand dictionnaire des caractères

Unicode est comme un **grand dictionnaire**, c’est ce **catalogue géant** où chaque caractère a un petit numéro de série appelé code point qui commence par U+.  
Genre :

- `🙂` = **U+1F642**
- `é` = **U+00E9**
- Et même le “𓂀” (un œil égyptien random) a son code, au cas où tu voudrais coder en hiéroglyphes dans ton SQL.

> Attention, Unicode **n’est pas un encodage**. C’est juste une liste avec des numéros. Ça te dit _qui_ tu es, pas _comment_ t’écrire sur disque. Bref, un zoo numérique.

Ton éditeur de texte manipule les caractères Unicode en mémoire. Tant que tu n’as pas **sauvegardé ton fichier**, le caractère `🙂` reste une valeur abstraite : l’éditeur sait que c’est un caractère Unicode valide (U+1F642) et peut demander à la police d’afficher le bon dessin.

Mais dès qu’il faut **enregistrer le fichier sur le disque**, il faut transformer ce numéro en **suite d’octets**. Parce que devine quoi ? Un fichier c’est juste un gros tas de 0 et de 1. C’est exactement le rôle de l’encodage. UTF-8 est un encodage, il prend ton `🙂` et le transforme en ce monstre : `F0 9F 99 82` pour pouvoir le stocker. Pas sexy, mais ça marche. Si tu ne précises rien, la plupart des éditeurs modernes choisissent **UTF-8** par défaut.

---
## 2. Comment UTF-8 encode les caractères

UTF-8 utilise un principe simple :

- Plus le numéro (code point) est petit, moins on a besoin d’octets.
- Plus il est grand, plus on ajoute d’octets.

### 2.1 Règles de base

- 1 octet → pour les 128 premiers caractères (ASCII)
- 2 octets → jusqu’à 11 bits de code
- 3 octets → jusqu’à 16 bits
- 4 octets → jusqu’à 21 bits (maximum de Unicode)

Exemple en binaire, U+1F642 = `0001 1111 0110 0100 0010` (20 bits utiles).  
20 bits ⇒ on a besoin de **4 octets**.

### 2.2 Modèle pour 4 octets

Le modèle pour 4 octets est 1 octet d'en tête et 3 octets de suite :
`11110xxx 10xxxxxx 10xxxxxx 10xxxxxx`

Pour 3 octets, l'octet d'en-tête serait `1110 xxxx`.  
Pour l'octet d'en-tête, le bit de poids fort est toujours égal à 1. Ensuite il y a autant de 1 que d'octets de suite (3) puis on ajoute des 0. Les `x` seront remplacés par les **bits du code point** (0001 1111 0110 0100 0010) de gauche à droite.

- Octet 1 (`11110xxx`) : prend les 2 bits les plus à gauche de `0001`, rempli les 2 derniers x
    - Résultat : `11110000` → `F0`
- Octet 2 (`10xxxxxx`) : prend les 6 bits suivants → `011111`
    - Résultat : `10011111` → `9F`
- Octet 3 (`10xxxxxx`) : prend les 6 bits suivants → `011001`
    - Résultat : `10011001` → `99`
- Octet 4 (`10xxxxxx`) : prend les bits restants → `000010`
    - Résultat : `10000010` → `82`

UTF-8 hex : `F0 9F 99 82`  
Binaire : `1111 0000 1001 1111 1001 1001 1000 0010`

---
## 3. Pourquoi l’encodage est crucial

Si tu lis ces 4 octets dans un fichier UTF-8, le parser les traduit en caractères Unicode (recompose les 20 bits et obtient **U+1F642**), affichant `🙂`.

Si l’encodage n’est pas le bon, c’est la tragédie.  
T’as sauvegardé ton fichier en UTF-8, et un autre logiciel débile essaie de l’ouvrir en ISO-8859-1. Quand il tombe sur `F0 9F 99 82`, il panique. Ton 🙂 se transforme en �.  
C’est plus un smiley, c’est le cri silencieux de ton âme.

C’est un peu comme si tu donnais les paroles d’une chanson de Jul à un traducteur latin → ça n’a plus aucun sens et tout le monde souffre.

> Moralité : la prochaine fois que tu vois un caractère chelou s’afficher en losange avec un point d’interrogation, ce n’est pas ton PC qui est possédé.  
> C’est juste toi qui as oublié de dire à tout le monde : **“on parle en UTF-8 ici, merci”**.
